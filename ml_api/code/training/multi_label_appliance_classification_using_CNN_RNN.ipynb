{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Td6R7HI7J4M"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import der notwendigen Bibliotheken für Datenverarbeitung, Machine Learning und Visualisierung\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.layers import GRU, Dropout, Dense, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "import seaborn as sns\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IjYaZy97c7y"
      },
      "source": [
        "## Datenvorbereitung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH70fTZubVnj"
      },
      "outputs": [],
      "source": [
        "# Mapping von Dateinamen zu Labels für die Geräteklassifikation\n",
        "device_mapping = {\n",
        "    \"washing_machine\": 0,\n",
        "    \"dishwasher\": 1,\n",
        "    \"freezer\": 2,\n",
        "    \"fridge\": 3,\n",
        "    \"micro_wave_oven\": 4\n",
        "}\n",
        "\n",
        "# Anzahl der Klassen basierend auf dem Mapping\n",
        "num_classes = len(device_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgPP0ovlbWPV"
      },
      "outputs": [],
      "source": [
        "# Liste der zu verwendenden CSV-Dateien\n",
        "selected_files = [\n",
        "    \"washing_machine_343_minimal_length.csv\",\n",
        "    \"dishwasher_53_minimal_length.csv\",\n",
        "    \"freezer_249_minimal_length.csv\",\n",
        "    \"fridge_98_minimal_length.csv\",\n",
        "    \"fridge_207_minimal_length.csv\",\n",
        "    \"fridge_284_minimal_length.csv\",\n",
        "    \"fridge_317_minimal_length.csv\",\n",
        "    \"micro_wave_oven_147_minimal_length.csv\",\n",
        "    \"micro_wave_oven_314_minimal_length.csv\",\n",
        "    \"washing_machine_32_minimal_length.csv\",\n",
        "    \"washing_machine_52_minimal_length.csv\",\n",
        "    \"washing_machine_135_minimal_length.csv\",\n",
        "    \"washing_machine_157_minimal_length.csv\",\n",
        "    \"washing_machine_218_minimal_length.csv\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD7W5lzVbWSW"
      },
      "outputs": [],
      "source": [
        "# Funktion zum Laden und Vorverarbeiten der Gerätedaten aus CSV-Dateien\n",
        "def load_device_data(file_path, label, peak_offset, scaler=None):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['label'] = label\n",
        "    df['peak_number'] += peak_offset\n",
        "    if scaler:\n",
        "        df['power'] = scaler.transform(df[['power']])\n",
        "    return df, df['peak_number'].max() + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhZX1LBhbauF"
      },
      "outputs": [],
      "source": [
        "# Initialisieren eines StandardScalers für die Normalisierung der Leistungsdaten\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq5IV_pZbWVI"
      },
      "outputs": [],
      "source": [
        "# Laden und Kombinieren aller Gerätedaten aus den CSV-Dateien\n",
        "data = []\n",
        "directory = \".\"\n",
        "peak_offset = 0\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "# Iterieren durch die ausgewählten Dateien und Laden der Daten\n",
        "for file_name in selected_files:\n",
        "    device_name = \"_\".join(file_name.split(\"_\")[:-3])\n",
        "    label = device_mapping.get(device_name)\n",
        "    if label is not None:\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        device_data, peak_offset = load_device_data(file_path, label, peak_offset)\n",
        "        all_data = pd.concat([all_data, device_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lj7ycEPbWX-"
      },
      "outputs": [],
      "source": [
        "# Fit den Scaler auf alle Leistungsdaten\n",
        "scaler.fit(all_data[['power']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP29gZy5bWa1"
      },
      "outputs": [],
      "source": [
        "# Laden der Gerätedaten nach Skalierung mit dem StandardScaler\n",
        "data = []\n",
        "peak_offset = 0\n",
        "\n",
        "for file_name in selected_files:\n",
        "    device_name = \"_\".join(file_name.split(\"_\")[:-3])\n",
        "    label = device_mapping.get(device_name)\n",
        "    if label is not None:\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        device_data, peak_offset = load_device_data(file_path, label, peak_offset, scaler=scaler)\n",
        "        data.append(device_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLfI2lxkbWdq"
      },
      "outputs": [],
      "source": [
        "# Kombinieren aller Daten in einem DataFrame\n",
        "df_all = pd.concat(data, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDOwi6yjbWgn"
      },
      "outputs": [],
      "source": [
        "# Gruppieren der Daten nach Peak-Nummer für die Zeitreihen\n",
        "grouped = df_all.groupby('peak_number')\n",
        "time_series = [group['power'].values for name, group in grouped]\n",
        "labels = [group['label'].iloc[0] for name, group in grouped]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBrlJE2ybWji"
      },
      "outputs": [],
      "source": [
        "# Konvertieren der Labels in eine binäre Matrix für die Multi-Label-Klassifikation\n",
        "labels = np.array([to_categorical(label, num_classes=num_classes) for label in labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBrWKxWObWmx"
      },
      "outputs": [],
      "source": [
        "# Bestimmen der maximalen Länge für das Padding der Zeitreihen\n",
        "max_length = max(len(ts) for ts in time_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqwVKTZj7tQs"
      },
      "source": [
        "## Datenaufbereitung für das Modell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyZxrw7pbWpE"
      },
      "outputs": [],
      "source": [
        "# Definition eines Data Generators für das Batch-Training\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, time_series, labels, batch_size, max_length):\n",
        "        self.time_series = time_series\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.time_series) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.time_series[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        # Anwenden von Padding auf die Zeitreihen\n",
        "        batch_x_padded = np.array([np.pad(ts, (0, self.max_length - len(ts)), 'constant') for ts in batch_x])\n",
        "        batch_x_padded = np.expand_dims(batch_x_padded, axis=-1)\n",
        "\n",
        "        return np.array(batch_x_padded), np.array(batch_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0aXJ3YzbWsm"
      },
      "outputs": [],
      "source": [
        "# Aufteilen der Zeitreihen-Daten in Trainings- und Testsets\n",
        "x_train, x_test, y_train, y_test = train_test_split(time_series, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMZGpd2rboWk"
      },
      "outputs": [],
      "source": [
        "# Berechnen der Klassen-Gewichte für das unbalancierte Dataset\n",
        "y_train_flat = np.argmax(y_train, axis=1)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_flat), y=y_train_flat)\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2gE8SiYboZg"
      },
      "outputs": [],
      "source": [
        "# Definition der Batch-Größe\n",
        "batch_size = 32\n",
        "train_gen = DataGenerator(x_train, y_train, batch_size, max_length)\n",
        "test_gen = DataGenerator(x_test, y_test, batch_size, max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My-fmn0q7y7I"
      },
      "source": [
        "## Modelldefinition und Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEvPRt41bocr"
      },
      "outputs": [],
      "source": [
        "# Definition eines CNN-RNN-Modells für die Klassifikation von Zeitreihen\n",
        "model = models.Sequential()\n",
        "\n",
        "# Convolutional Layer für Feature-Extraktion\n",
        "model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(max_length, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Bidirektionale GRU-Schichten\n",
        "model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Bidirectional(GRU(256)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Dense Layer mit Regularisierung\n",
        "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(num_classes, activation='sigmoid'))  # Sigmoid für Multi-Label-Klassifikation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qBTtapzbWwd"
      },
      "outputs": [],
      "source": [
        "# Kompilieren des Modells mit Adam Optimizer und Loss für Multi-Label Klassifikation\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d064Yeembvf2"
      },
      "outputs": [],
      "source": [
        "# Lernratenscheduler und Early Stopping\n",
        "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001)\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec6AAyfwbvjC"
      },
      "outputs": [],
      "source": [
        "# Trainieren des Modells\n",
        "history = model.fit(train_gen, epochs=100, validation_data=test_gen, callbacks=[lr_scheduler, early_stopping], class_weight=class_weight_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJbhW39B73gt"
      },
      "source": [
        "## Modellbewertung und Visualisierung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2ACmUh9bzKi"
      },
      "outputs": [],
      "source": [
        "# Bewertung des Modells auf den Testdaten\n",
        "test_loss, test_acc = model.evaluate(test_gen)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BChqduUbzN4"
      },
      "outputs": [],
      "source": [
        "# Plotten des Trainings- und Validierungsverlusts\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG_8xT2sbzQ9"
      },
      "outputs": [],
      "source": [
        "# Modell speichern\n",
        "model.save(\"appliance_classification_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MaFDiqObzUO"
      },
      "outputs": [],
      "source": [
        "# Modell laden\n",
        "model = load_model('appliance_classification_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qdj3oRUb6SG"
      },
      "outputs": [],
      "source": [
        "# Vorhersagen auf dem gesamten Testdatensatz in einem Schritt sammeln\n",
        "x_test_all, y_test_all = test_gen[0]\n",
        "for i in range(1, len(test_gen)):\n",
        "    batch_x, batch_y = test_gen[i]\n",
        "    x_test_all = np.vstack((x_test_all, batch_x))\n",
        "    y_test_all = np.vstack((y_test_all, batch_y))\n",
        "\n",
        "y_pred_all = model.predict(x_test_all)\n",
        "y_pred_rounded = np.round(y_pred_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poEP5D4db6Wv"
      },
      "outputs": [],
      "source": [
        "# Generieren eines Classification Reports\n",
        "report = classification_report(y_test_all, y_pred_rounded, target_names=device_mapping.keys())\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Berechnung der Multilabel Confusion Matrix\n",
        "conf_matrix = multilabel_confusion_matrix(y_test_all, y_pred_rounded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5psVCFpb9MF"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix für jedes Gerät plotten\n",
        "for i, (label, matrix) in enumerate(zip(device_mapping.keys(), conf_matrix)):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Not \" + label, label], yticklabels=[\"Not \" + label, label])\n",
        "    plt.title(f'Confusion Matrix for {label}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
